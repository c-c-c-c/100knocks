{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\"\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32768)             1081344   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 16, 16, 256)       819456    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 32, 32, 256)       1048832   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 256)       1638656   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 256)       1638656   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 32, 32, 3)         37635     \n",
      "=================================================================\n",
      "Total params: 6,264,579\n",
      "Trainable params: 6,264,579\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#リスト8-29:GANの生成者ネットワーク\n",
    "import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "\n",
    "latent_dim = 32 #最初に設定する潜在空間ベクトル次元\n",
    "height = 32 #縦32ピクセル\n",
    "width = 32 #横32ピクセル\n",
    "channels = 3 #色3チャネル\n",
    "\n",
    "#入力変数(inputは正規分布のランダムノイズ)\n",
    "generator_input = keras.Input(shape=(latent_dim,))\n",
    "\n",
    "#入力を16×16、128チャネルの特徴マップに変換\n",
    "x = layers.Dense(128 * 16 * 16)(generator_input)\n",
    "x = layers.LeakyReLU()(x) #活性化関数は\"LeakyReLU\"を使用\n",
    "x = layers.Reshape((16, 16, 128))(x)\n",
    "\n",
    "#畳み込み層を追加(畳込みカーネルサイズは5)\n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "#32×32にアップサンプリング(畳込みカーネルサイズは4)\n",
    "x = layers.Conv2DTranspose(256, 4, strides=2, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "#さらに畳み込み層を追加(畳込みカーネルサイズは5)\n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "#32×32、3(channels)チャネル（CIFAR10の画像の形状）の特徴マップを生成(畳込みカーネルサイズは7)\n",
    "x = layers.Conv2D(channels, 7, activation='tanh', padding='same')(x) #最後の活性化関数は\"tanh\"を使用\n",
    "\n",
    "#形状がlatent_dim(32)の入力を、形状がx(32,32,3)の画像にマッピング\n",
    "generator = keras.models.Model(generator_input, x) #\"generator\"に生成者ネットワークを格納\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 30, 30, 128)       3584      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 30, 30, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 14, 14, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 6, 6, 128)         262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 2, 2, 128)         262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 790,913\n",
      "Trainable params: 790,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#リスト8-30:GANの判別者ネットワーク\n",
    "\n",
    "#入力変数(shapeは(縦32,横32,カラーチャネル3))\n",
    "discriminator_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "x = layers.Conv2D(128, 3)(discriminator_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(128, 4, strides=2)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(128, 4, strides=2)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(128, 4, strides=2)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "#ドロップアウト層を１つ追加:重要なトリック!\n",
    "x = layers.Dropout(0.4)(x)\n",
    "\n",
    "#全結合分類層(2値分類)\n",
    "x = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "#disctriminatorモデルをインスタンス化:形状が(32,32,3)の入力で2値分類(fake/real)を実行\n",
    "discriminator = keras.models.Model(discriminator_input, x) #\"discriminator\"に判別者ネットワークを格納\n",
    "discriminator.summary()\n",
    "\n",
    "#オプティマイザで勾配刈込を使用し（clipvalue）、訓練を安定させるために学習率減衰を使用(decay)\n",
    "discriminator_optimizer = keras.optimizers.RMSprop(lr=0.0008,\n",
    "                                                   clipvalue=1.0, #（勾配の）絶対値が1.0を超えた時、勾配がクリップ（切り取り）されます(勾配爆発を防ぐ)\n",
    "                                                   decay=1e-8) #各更新の学習率減衰\n",
    "discriminator.compile(optimizer=discriminator_optimizer, loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#リスト8-31:敵対者ネットワーク(生成者と判別者をつなぎ合わせる)\n",
    "discriminator.trainable = False #discriminatorを凍結するため、重みの更新不可能に設定(これはganモデルにのみ適用される)\n",
    "\n",
    "gan_input = keras.Input(shape=(latent_dim,)) #gan_inputに生成者ネットワークの入力(正規分布のランダムノイズ)\n",
    "gan_output = discriminator(generator(gan_input)) #gan_outputに判別者ネットワークの出力(2値分類)\n",
    "gan = keras.models.Model(gan_input, gan_output) #\"gan\"に生成者ネットワークのランダムノイズを入力し、判別者ネットワークでの2値分類を出力するモデルを格納\n",
    "\n",
    "gan_optimizer = keras.optimizers.RMSprop(lr=0.0004, clipvalue=1.0, decay=1e-8)\n",
    "gan.compile(optimizer=gan_optimizer, loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator loss: 0.7049017\n",
      "adversarial loss: 0.6673985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator loss: 0.74159485\n",
      "adversarial loss: 0.9378754\n",
      "discriminator loss: 0.70603085\n",
      "adversarial loss: 0.8391715\n",
      "discriminator loss: 0.7147874\n",
      "adversarial loss: 0.7847269\n",
      "discriminator loss: 0.67453337\n",
      "adversarial loss: 0.7909132\n",
      "discriminator loss: 0.6900134\n",
      "adversarial loss: 0.79012215\n",
      "discriminator loss: 0.6988419\n",
      "adversarial loss: 0.7405921\n",
      "discriminator loss: 0.68553\n",
      "adversarial loss: 0.75800526\n",
      "discriminator loss: 0.70742035\n",
      "adversarial loss: 0.713962\n",
      "discriminator loss: 0.69843376\n",
      "adversarial loss: 0.73864704\n",
      "discriminator loss: 0.69773877\n",
      "adversarial loss: 0.80400944\n",
      "discriminator loss: 0.6914603\n",
      "adversarial loss: 0.70939744\n",
      "discriminator loss: 0.6990951\n",
      "adversarial loss: 0.7305231\n",
      "discriminator loss: 0.6936127\n",
      "adversarial loss: 0.7225195\n",
      "discriminator loss: 0.68484116\n",
      "adversarial loss: 0.73860776\n",
      "discriminator loss: 0.7216835\n",
      "adversarial loss: 0.82687837\n",
      "discriminator loss: 0.6998333\n",
      "adversarial loss: 0.72169316\n",
      "discriminator loss: 0.71403235\n",
      "adversarial loss: 0.74646825\n",
      "discriminator loss: 0.7005736\n",
      "adversarial loss: 0.7771285\n",
      "discriminator loss: 0.6779416\n",
      "adversarial loss: 1.5041265\n",
      "discriminator loss: 0.684044\n",
      "adversarial loss: 0.7621089\n",
      "discriminator loss: 0.70556426\n",
      "adversarial loss: 0.72938144\n",
      "discriminator loss: 0.67994535\n",
      "adversarial loss: 0.898105\n",
      "discriminator loss: 0.7022378\n",
      "adversarial loss: 0.67984647\n",
      "discriminator loss: 0.7073258\n",
      "adversarial loss: 0.72476757\n",
      "discriminator loss: 0.68892854\n",
      "adversarial loss: 0.7217415\n",
      "discriminator loss: 0.6848714\n",
      "adversarial loss: 0.78180015\n",
      "discriminator loss: 0.70061415\n",
      "adversarial loss: 0.73079956\n",
      "discriminator loss: 0.66216373\n",
      "adversarial loss: 0.79284734\n",
      "discriminator loss: 0.7053653\n",
      "adversarial loss: 0.743553\n",
      "discriminator loss: 0.686226\n",
      "adversarial loss: 0.731928\n",
      "discriminator loss: 0.6945713\n",
      "adversarial loss: 0.73033607\n",
      "discriminator loss: 0.69310665\n",
      "adversarial loss: 0.759356\n",
      "discriminator loss: 0.69621193\n",
      "adversarial loss: 0.77044785\n",
      "discriminator loss: 0.6911383\n",
      "adversarial loss: 0.7618691\n",
      "discriminator loss: 0.6865227\n",
      "adversarial loss: 0.7479946\n",
      "discriminator loss: 0.7034036\n",
      "adversarial loss: 0.6859181\n",
      "discriminator loss: 0.69014895\n",
      "adversarial loss: 0.79103345\n",
      "discriminator loss: 0.6861659\n",
      "adversarial loss: 1.2063407\n",
      "discriminator loss: 0.68409604\n",
      "adversarial loss: 0.7353624\n",
      "discriminator loss: 0.6874927\n",
      "adversarial loss: 0.7532234\n",
      "discriminator loss: 0.69497234\n",
      "adversarial loss: 0.75450057\n",
      "discriminator loss: 0.6874684\n",
      "adversarial loss: 0.7322561\n",
      "discriminator loss: 0.70487535\n",
      "adversarial loss: 0.7049831\n",
      "discriminator loss: 0.6753165\n",
      "adversarial loss: 0.76016426\n",
      "discriminator loss: 0.6956868\n",
      "adversarial loss: 0.73947185\n",
      "discriminator loss: 0.69298697\n",
      "adversarial loss: 0.78496456\n",
      "discriminator loss: 0.69434375\n",
      "adversarial loss: 0.75067246\n",
      "discriminator loss: 0.6880796\n",
      "adversarial loss: 0.7257492\n",
      "discriminator loss: 0.6877548\n",
      "adversarial loss: 0.744582\n",
      "discriminator loss: 0.6907147\n",
      "adversarial loss: 0.7128073\n",
      "discriminator loss: 0.6936892\n",
      "adversarial loss: 0.7527031\n",
      "discriminator loss: 0.6956215\n",
      "adversarial loss: 0.7381311\n",
      "discriminator loss: 0.6976267\n",
      "adversarial loss: 0.75468886\n",
      "discriminator loss: 0.69072264\n",
      "adversarial loss: 0.7480429\n",
      "discriminator loss: 0.7277037\n",
      "adversarial loss: 1.0651401\n",
      "discriminator loss: 0.6873735\n",
      "adversarial loss: 0.73469144\n",
      "discriminator loss: 0.702932\n",
      "adversarial loss: 0.76775396\n",
      "discriminator loss: 0.7016684\n",
      "adversarial loss: 0.70516765\n",
      "discriminator loss: 0.69531137\n",
      "adversarial loss: 0.8122031\n",
      "discriminator loss: 0.67206717\n",
      "adversarial loss: 0.67461\n",
      "discriminator loss: 0.68610466\n",
      "adversarial loss: 0.78538465\n",
      "discriminator loss: 0.669511\n",
      "adversarial loss: 0.4065501\n",
      "discriminator loss: 0.6974399\n",
      "adversarial loss: 0.82930624\n",
      "discriminator loss: 0.65940934\n",
      "adversarial loss: 0.86039245\n",
      "discriminator loss: 0.705948\n",
      "adversarial loss: 0.6791277\n",
      "discriminator loss: 0.6923164\n",
      "adversarial loss: 0.75714815\n",
      "discriminator loss: 0.73204774\n",
      "adversarial loss: 1.0217255\n",
      "discriminator loss: 0.72377086\n",
      "adversarial loss: 0.8009664\n",
      "discriminator loss: 0.6870585\n",
      "adversarial loss: 0.7886621\n",
      "discriminator loss: 0.6714574\n",
      "adversarial loss: 0.9132516\n",
      "discriminator loss: 0.6533316\n",
      "adversarial loss: 0.7757629\n",
      "discriminator loss: 0.7426471\n",
      "adversarial loss: 0.6981128\n",
      "discriminator loss: 0.6963664\n",
      "adversarial loss: 0.7671359\n",
      "discriminator loss: 0.6654657\n",
      "adversarial loss: 0.81634885\n",
      "discriminator loss: 0.67003477\n",
      "adversarial loss: 0.6099973\n",
      "discriminator loss: 0.66046304\n",
      "adversarial loss: 0.8176549\n",
      "discriminator loss: 0.67072\n",
      "adversarial loss: 0.79082817\n",
      "discriminator loss: 0.69005173\n",
      "adversarial loss: 0.80185354\n",
      "discriminator loss: 0.74890935\n",
      "adversarial loss: 0.7775637\n",
      "discriminator loss: 0.7035728\n",
      "adversarial loss: 0.7971104\n"
     ]
    }
   ],
   "source": [
    "#リスト8-32:GANの訓練の実装\n",
    "from keras.preprocessing import image\n",
    "from tensorflow.keras.datasets import cifar10 #cifar10を読み込むため追加したコード\n",
    "\n",
    "(x_train, y_train), (_, _) = cifar10.load_data() #cifar10を読み込むため追加したコード\n",
    "\n",
    "#カエルの画像（クラス６）を選択\n",
    "x_train = x_train[y_train.flatten() == 6]\n",
    "\n",
    "#データを正規化\n",
    "x_train = x_train.reshape(\n",
    "    (x_train.shape[0],) +\n",
    "    (height, width, channels)).astype('float32') / 255.\n",
    "\n",
    "iterations = 10000\n",
    "batch_size = 20\n",
    "\n",
    "#生成された画像の保存先を指定\n",
    "save_dir = '8gan' #cifar10データセットを学内サーバで使用するため追加したコード\n",
    "\n",
    "\n",
    "############################## 学習 #################################\n",
    "start = 0\n",
    "for step in range(iterations): #訓練ループを開始(10000回)\n",
    "    \n",
    "    #dircriminatorモデルの訓練用に潜在空間から点をランダムに抽出➀(正規分布のランダムノイズ, shapeは(20,32))\n",
    "    random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n",
    "    \n",
    "    #generatorモデルでランダムノイズ(20,32)を形状(20,32,32,3)の画像にマッピング\n",
    "    generated_images = generator.predict(random_latent_vectors)\n",
    "\n",
    "    #本物の画像と組み合わせる\n",
    "    stop = start + batch_size #一回につきバッチサイズ20ずつ足す\n",
    "    real_images = x_train[start: stop] #一回につきバッチサイズ20ずつ本物の画像を抽出(形状を偽物の(20,32,32,3)に合わせる)\n",
    "    combined_images = np.concatenate([generated_images, real_images]) #本物と偽物の画像を連結する(形状が(40,32,32,3))\n",
    "\n",
    "    #本物の画像と偽物の画像を区別するラベルを立てる。　combined_imagesデータに合わせ、最初の20個の偽物は1ベクトル×20(20,1)、\n",
    "    #残りの20個の本物は0ベクトル×20(20,1)とし、それらを連結して正解ラベル×40(40,1)を作る)\n",
    "    labels = np.concatenate([np.ones((batch_size, 1)), np.zeros((batch_size, 1))])\n",
    "    \n",
    "    #ラベルにランダムノイズを追加:重要なトリック！\n",
    "    labels += 0.05 * np.random.random(labels.shape)\n",
    "\n",
    "    #\"discriminator\"モデルを訓練(.train_on_batchで単一のバッチを使用して1回だけトレーニングする)\n",
    "    d_loss = discriminator.train_on_batch(combined_images, labels) #入力には本物、偽物の画像を連結した画像データ、出力には同様の正解ラベル\n",
    "\n",
    "    #dircriminatorモデルの訓練用に潜在空間から点をランダムに抽出➁(正規分布のランダムノイズ, shapeは(20,32))\n",
    "    random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n",
    "\n",
    "    #「これらはすべて本物の画像」であることを示すラベルを立てる(本物なので0ベクトル×20(20,1))\n",
    "    misleading_targets = np.zeros((batch_size, 1))\n",
    "\n",
    "    #\"gan\"モデルを通じてgeneratorを訓練(ganモデルではdiscriminatorの重みが凍結される)\n",
    "    #訓練データの入力は偽物画像（ランダムノイズ）だが、出力(目的値)は全て本物の画像(0ベクトル)であるとして\n",
    "    #嘘を付いた形の訓練をすることで、偽物画像が本物画像に近づく変換をするモデルに適合していく\n",
    "    a_loss = gan.train_on_batch(random_latent_vectors, misleading_targets)\n",
    "\n",
    "    #訓練1回終わったら次の訓練では今回使用したものの20枚後の画像データを使用\n",
    "    start += batch_size\n",
    "    \n",
    "    #訓練に全てのカエル画像データ5000枚を使用し終えたら(x_train[4980:5000])、また最初の20枚の訓練データを使用して訓練(x_train[0:20])\n",
    "    if start > len(x_train) - batch_size:\n",
    "      start = 0\n",
    "    \n",
    "    ##################### 訓練100回おきに保存とプロット ######################\n",
    "    if step % 100 == 0:\n",
    "        gan.save_weights('gan.h5') #モデルの重みを保存\n",
    "\n",
    "        #\"discriminator\"および\"gan\"モデルの訓練誤差(binary_crossentropy)を出力\n",
    "        print('discriminator loss:', d_loss)\n",
    "        print('adversarial loss:', a_loss)\n",
    "\n",
    "        #生成された画像を1つずつ保存(generated_frog0.ping, generated_frog100.ping,..., generated_frog9900.ping)\n",
    "        img = image.array_to_img(generated_images[0] * 255., scale=False)\n",
    "        img.save(os.path.join(save_dir, 'generated_frog' + str(step) + '.png'))\n",
    "\n",
    "        #比較のために本物の画像を1つずつ保存(real_frog0.ping, real_frog100.ping,..., real_frog9900.ping)\n",
    "        img = image.array_to_img(real_images[0] * 255., scale=False)\n",
    "        img.save(os.path.join(save_dir, 'real_frog' + str(step) + '.png'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
