{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_extraction_with_bert_yoshimura",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "600d36306fac41a3a522929ce58c4a3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b268b2b83e94404fa2ccfa00cc4bff6d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0930c9b6f7b742beb6d971a55f98815f",
              "IPY_MODEL_3f5c75b49af946c08e4fcd2980b7e433"
            ]
          }
        },
        "b268b2b83e94404fa2ccfa00cc4bff6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0930c9b6f7b742beb6d971a55f98815f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_800fd7fcb348490285f37ff3ed6b5927",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_35d41b3d48b54ef1bdf03b88d47f87b4"
          }
        },
        "3f5c75b49af946c08e4fcd2980b7e433": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e08b71856e6f4e51923e33cb1c2f38bc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 1.32MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d43baa257acf493c8b81691e77260f1b"
          }
        },
        "800fd7fcb348490285f37ff3ed6b5927": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "35d41b3d48b54ef1bdf03b88d47f87b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e08b71856e6f4e51923e33cb1c2f38bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d43baa257acf493c8b81691e77260f1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8d19bfb3bc1f481ebca4be5d92ad0ec8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_34ccd19f3e254e7aae08c116a430ecbf",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2c79d402993740fe920a7626aa97de78",
              "IPY_MODEL_c1d1a69be84a4a66a1c77d7d05194275"
            ]
          }
        },
        "34ccd19f3e254e7aae08c116a430ecbf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2c79d402993740fe920a7626aa97de78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5a58ef860b6246c38a5b07e2e738205f",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3e778590b5b845e2985e30289010d1d0"
          }
        },
        "c1d1a69be84a4a66a1c77d7d05194275": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_96fb294fcf70425cb0a541e1ec4f74fb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:08&lt;00:00, 50.4B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_70db864c86b04e1cb8884b2596c5d4c7"
          }
        },
        "5a58ef860b6246c38a5b07e2e738205f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3e778590b5b845e2985e30289010d1d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "96fb294fcf70425cb0a541e1ec4f74fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "70db864c86b04e1cb8884b2596c5d4c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fe6e46527d1b43e8992c246d4eb17221": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8fe616af4f5d4082b6299c42114252ad",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8ad7ac68c6f444468aa893315236ff8b",
              "IPY_MODEL_e07328c697e34acca8b8fceced184277"
            ]
          }
        },
        "8fe616af4f5d4082b6299c42114252ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8ad7ac68c6f444468aa893315236ff8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6c61595c2f59436cab49322e30167252",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 536063208,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 536063208,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b78aa9c1c00f4813b59fa98a1d0027c2"
          }
        },
        "e07328c697e34acca8b8fceced184277": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_12c91e18af5e455fb66f6ff1c95af04c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 536M/536M [00:08&lt;00:00, 65.2MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2a9d11ef070745ffa733748ac715ed83"
          }
        },
        "6c61595c2f59436cab49322e30167252": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b78aa9c1c00f4813b59fa98a1d0027c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "12c91e18af5e455fb66f6ff1c95af04c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2a9d11ef070745ffa733748ac715ed83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/c-c-c-c/100knocks/blob/master/text_extraction_with_bert_yoshimura.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Yqm-f7uXbh9"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This demonstration uses SQuAD (Stanford Question-Answering Dataset).\n",
        "In SQuAD, an input consists of a question, and a paragraph for context.\n",
        "The goal is to find the span of text in the paragraph that answers the question.\n",
        "We evaluate our performance on this data with the \"Exact Match\" metric,\n",
        "which measures the percentage of predictions that exactly match any one of the\n",
        "ground-truth answers.\n",
        "\n",
        "We fine-tune a BERT model to perform this task as follows:\n",
        "\n",
        "1. Feed the context and the question as inputs to BERT.\n",
        "2. Take two vectors S and T with dimensions equal to that of\n",
        "   hidden states in BERT.\n",
        "3. Compute the probability of each token being the start and end of\n",
        "   the answer span. The probability of a token being the start of\n",
        "   the answer is given by a dot product between S and the representation\n",
        "   of the token in the last layer of BERT, followed by a softmax over all tokens.\n",
        "   The probability of a token being the end of the answer is computed\n",
        "   similarly with the vector T.\n",
        "4. Fine-tune BERT and learn S and T along the way.\n",
        "\n",
        "**References:**\n",
        "\n",
        "- [BERT](https://arxiv.org/pdf/1810.04805.pdf)\n",
        "- [SQuAD](https://arxiv.org/abs/1606.05250)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpAdOkWjXbh5"
      },
      "source": [
        "# BERT (from HuggingFace Transformers) for Text Extraction\n",
        "\n",
        "**Author:** [Apoorv Nandan](https://twitter.com/NandanApoorv)<br>\n",
        "**Date created:** 2020/05/23<br>\n",
        "**Last modified:** 2020/05/23<br>\n",
        "**Description:** Fine tune pretrained BERT from HuggingFace Transformers on SQuAD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTFgTvlzXbh9"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvMOM4Olkwu4",
        "outputId": "d53f983a-e800-4b5f-d1b7-64a4cc632164"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/0c/7d5950fcd80b029be0a8891727ba21e0cd27692c407c51261c3c921f6da3/transformers-4.1.1-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 18.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 27.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=472525d29305003dff7dbbf1e473846a518cc0a23044b647111c620c5e1e6f73\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBOeYbj5Xbh-"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import string\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
        "\n",
        "max_len = 384\n",
        "configuration = BertConfig()  # default parameters and configuration for BERT\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFW-pfAaXbh-"
      },
      "source": [
        "## Set-up BERT tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PT5gT1n0Xbh-",
        "outputId": "08ca47ac-7b77-432a-df71-df6a3ea64a03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "600d36306fac41a3a522929ce58c4a3a",
            "b268b2b83e94404fa2ccfa00cc4bff6d",
            "0930c9b6f7b742beb6d971a55f98815f",
            "3f5c75b49af946c08e4fcd2980b7e433",
            "800fd7fcb348490285f37ff3ed6b5927",
            "35d41b3d48b54ef1bdf03b88d47f87b4",
            "e08b71856e6f4e51923e33cb1c2f38bc",
            "d43baa257acf493c8b81691e77260f1b"
          ]
        }
      },
      "source": [
        "# Save the slow pretrained tokenizer\n",
        "slow_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "save_path = \"bert_base_uncased/\"\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "slow_tokenizer.save_pretrained(save_path)\n",
        "\n",
        "# Load the fast tokenizer from saved file\n",
        "tokenizer = BertWordPieceTokenizer(\"bert_base_uncased/vocab.txt\", lowercase=True)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "600d36306fac41a3a522929ce58c4a3a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nsow50YaXbh-"
      },
      "source": [
        "## Load the data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxmTQVljXbh_",
        "outputId": "0cbe4bbd-0735-4c77-9510-90f56118b714",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_data_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\"\n",
        "train_path = keras.utils.get_file(\"train.json\", train_data_url)\n",
        "eval_data_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\"\n",
        "eval_path = keras.utils.get_file(\"eval.json\", eval_data_url)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\n",
            "30294016/30288272 [==============================] - 0s 0us/step\n",
            "Downloading data from https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\n",
            "4857856/4854279 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUran-7lXbh_"
      },
      "source": [
        "## Preprocess the data\n",
        "\n",
        "1. Go through the JSON file and store every record as a `SquadExample` object.\n",
        "2. Go through each `SquadExample` and create `x_train, y_train, x_eval, y_eval`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCYo89ojXbh_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54bf365b-b9e1-4b14-aaf2-e49bae332b7f"
      },
      "source": [
        "\n",
        "class SquadExample:\n",
        "    def __init__(self, question, context, start_char_idx, answer_text, all_answers):\n",
        "        self.question = question\n",
        "        self.context = context\n",
        "        self.start_char_idx = start_char_idx\n",
        "        self.answer_text = answer_text\n",
        "        self.all_answers = all_answers\n",
        "        self.skip = False\n",
        "\n",
        "    def preprocess(self):\n",
        "        context = self.context\n",
        "        question = self.question\n",
        "        answer_text = self.answer_text\n",
        "        start_char_idx = self.start_char_idx\n",
        "\n",
        "        # Clean context, answer and question\n",
        "        context = \" \".join(str(context).split())\n",
        "        question = \" \".join(str(question).split())\n",
        "        answer = \" \".join(str(answer_text).split())\n",
        "\n",
        "        # Find end character index of answer in context\n",
        "        end_char_idx = start_char_idx + len(answer)\n",
        "        if end_char_idx >= len(context):\n",
        "            self.skip = True\n",
        "            return\n",
        "\n",
        "        # Mark the character indexes in context that are in answer\n",
        "        is_char_in_ans = [0] * len(context)\n",
        "        for idx in range(start_char_idx, end_char_idx):\n",
        "            is_char_in_ans[idx] = 1\n",
        "\n",
        "        # Tokenize context\n",
        "        tokenized_context = tokenizer.encode(context)\n",
        "\n",
        "        # Find tokens that were created from answer characters\n",
        "        ans_token_idx = []\n",
        "        for idx, (start, end) in enumerate(tokenized_context.offsets):\n",
        "            if sum(is_char_in_ans[start:end]) > 0:\n",
        "                ans_token_idx.append(idx)\n",
        "\n",
        "        if len(ans_token_idx) == 0:\n",
        "            self.skip = True\n",
        "            return\n",
        "\n",
        "        # Find start and end token index for tokens from answer\n",
        "        start_token_idx = ans_token_idx[0]\n",
        "        end_token_idx = ans_token_idx[-1]\n",
        "\n",
        "        # Tokenize question\n",
        "        tokenized_question = tokenizer.encode(question)\n",
        "\n",
        "        # Create inputs\n",
        "        input_ids = tokenized_context.ids + tokenized_question.ids[1:]\n",
        "        token_type_ids = [0] * len(tokenized_context.ids) + [1] * len(\n",
        "            tokenized_question.ids[1:]\n",
        "        )\n",
        "        attention_mask = [1] * len(input_ids)\n",
        "\n",
        "        # Pad and create attention masks.\n",
        "        # Skip if truncation is needed\n",
        "        padding_length = max_len - len(input_ids)\n",
        "        if padding_length > 0:  # pad\n",
        "            input_ids = input_ids + ([0] * padding_length)\n",
        "            attention_mask = attention_mask + ([0] * padding_length)\n",
        "            token_type_ids = token_type_ids + ([0] * padding_length)\n",
        "        elif padding_length < 0:  # skip\n",
        "            self.skip = True\n",
        "            return\n",
        "\n",
        "        self.input_ids = input_ids\n",
        "        self.token_type_ids = token_type_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.start_token_idx = start_token_idx\n",
        "        self.end_token_idx = end_token_idx\n",
        "        self.context_token_to_char = tokenized_context.offsets\n",
        "\n",
        "\n",
        "with open(train_path) as f:\n",
        "    raw_train_data = json.load(f)\n",
        "\n",
        "with open(eval_path) as f:\n",
        "    raw_eval_data = json.load(f)\n",
        "\n",
        "\n",
        "def create_squad_examples(raw_data):\n",
        "    squad_examples = []\n",
        "    for item in raw_data[\"data\"]:\n",
        "        for para in item[\"paragraphs\"]:\n",
        "            context = para[\"context\"]\n",
        "            for qa in para[\"qas\"]:\n",
        "                question = qa[\"question\"]\n",
        "                answer_text = qa[\"answers\"][0][\"text\"]\n",
        "                all_answers = [_[\"text\"] for _ in qa[\"answers\"]]\n",
        "                start_char_idx = qa[\"answers\"][0][\"answer_start\"]\n",
        "                squad_eg = SquadExample(\n",
        "                    question, context, start_char_idx, answer_text, all_answers\n",
        "                )\n",
        "                squad_eg.preprocess()\n",
        "                squad_examples.append(squad_eg)\n",
        "    return squad_examples\n",
        "\n",
        "\n",
        "def create_inputs_targets(squad_examples):\n",
        "    dataset_dict = {\n",
        "        \"input_ids\": [],\n",
        "        \"token_type_ids\": [],\n",
        "        \"attention_mask\": [],\n",
        "        \"start_token_idx\": [],\n",
        "        \"end_token_idx\": [],\n",
        "    }\n",
        "    for item in squad_examples:\n",
        "        if item.skip == False:\n",
        "            for key in dataset_dict:\n",
        "                dataset_dict[key].append(getattr(item, key))\n",
        "    for key in dataset_dict:\n",
        "        dataset_dict[key] = np.array(dataset_dict[key])\n",
        "\n",
        "    x = [\n",
        "        dataset_dict[\"input_ids\"],\n",
        "        dataset_dict[\"token_type_ids\"],\n",
        "        dataset_dict[\"attention_mask\"],\n",
        "    ]\n",
        "    y = [dataset_dict[\"start_token_idx\"], dataset_dict[\"end_token_idx\"]]\n",
        "    return x, y\n",
        "\n",
        "\n",
        "train_squad_examples = create_squad_examples(raw_train_data)\n",
        "x_train, y_train = create_inputs_targets(train_squad_examples)\n",
        "print(f\"{len(train_squad_examples)} training points created.\")\n",
        "\n",
        "eval_squad_examples = create_squad_examples(raw_eval_data)\n",
        "x_eval, y_eval = create_inputs_targets(eval_squad_examples)\n",
        "print(f\"{len(eval_squad_examples)} evaluation points created.\")\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "87599 training points created.\n",
            "10570 evaluation points created.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeGNeX6rXbiC"
      },
      "source": [
        "Create the Question-Answering Model using BERT and Functional API\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpObv992XbiE"
      },
      "source": [
        "\n",
        "def create_model():\n",
        "    ## BERT encoder\n",
        "    encoder = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "    ## QA Model\n",
        "    input_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
        "    token_type_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
        "    attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
        "    embedding = encoder(\n",
        "        input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask\n",
        "    )[0]\n",
        "\n",
        "    start_logits = layers.Dense(1, name=\"start_logit\", use_bias=False)(embedding)\n",
        "    start_logits = layers.Flatten()(start_logits)\n",
        "\n",
        "    end_logits = layers.Dense(1, name=\"end_logit\", use_bias=False)(embedding)\n",
        "    end_logits = layers.Flatten()(end_logits)\n",
        "\n",
        "    start_probs = layers.Activation(keras.activations.softmax)(start_logits)\n",
        "    end_probs = layers.Activation(keras.activations.softmax)(end_logits)\n",
        "\n",
        "    model = keras.Model(\n",
        "        inputs=[input_ids, token_type_ids, attention_mask],\n",
        "        outputs=[start_probs, end_probs],\n",
        "    )\n",
        "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "    optimizer = keras.optimizers.Adam(lr=5e-5)\n",
        "    model.compile(optimizer=optimizer, loss=[loss, loss])\n",
        "    return model\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcvB_jz_XbiE"
      },
      "source": [
        "This code should preferably be run on Google Colab TPU runtime.\n",
        "With Colab TPUs, each epoch will take 5-6 minutes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHFmmnM2XbiE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8d19bfb3bc1f481ebca4be5d92ad0ec8",
            "34ccd19f3e254e7aae08c116a430ecbf",
            "2c79d402993740fe920a7626aa97de78",
            "c1d1a69be84a4a66a1c77d7d05194275",
            "5a58ef860b6246c38a5b07e2e738205f",
            "3e778590b5b845e2985e30289010d1d0",
            "96fb294fcf70425cb0a541e1ec4f74fb",
            "70db864c86b04e1cb8884b2596c5d4c7",
            "fe6e46527d1b43e8992c246d4eb17221",
            "8fe616af4f5d4082b6299c42114252ad",
            "8ad7ac68c6f444468aa893315236ff8b",
            "e07328c697e34acca8b8fceced184277",
            "6c61595c2f59436cab49322e30167252",
            "b78aa9c1c00f4813b59fa98a1d0027c2",
            "12c91e18af5e455fb66f6ff1c95af04c",
            "2a9d11ef070745ffa733748ac715ed83"
          ]
        },
        "outputId": "4b4f7488-7172-41c4-b2eb-ca7af616b5ca"
      },
      "source": [
        "use_tpu = True\n",
        "if use_tpu:\n",
        "    # Create distribution strategy\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "\n",
        "    # Create model\n",
        "    with strategy.scope():\n",
        "        model = create_model()\n",
        "else:\n",
        "    model = create_model()\n",
        "\n",
        "model.summary()\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Entering into master device scope: /job:worker/replica:0/task:0/device:CPU:0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.23.62.218:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.23.62.218:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n",
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8d19bfb3bc1f481ebca4be5d92ad0ec8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fe6e46527d1b43e8992c246d4eb17221",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=536063208.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fd62fedb5f8>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: <cyfunction Socket.send at 0x7fd647726e58> is not a module, class, method, function, traceback, frame, or code object\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fd62fedb5f8>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: <cyfunction Socket.send at 0x7fd647726e58> is not a module, class, method, function, traceback, frame, or code object\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fd62fedb5f8>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: <cyfunction Socket.send at 0x7fd647726e58> is not a module, class, method, function, traceback, frame, or code object\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7fd6450b98c8> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7fd6450b98c8> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function wrap at 0x7fd6450b98c8> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 384)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 384)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 384)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_bert_model (TFBertModel)     TFBaseModelOutputWit 109482240   input_1[0][0]                    \n",
            "                                                                 input_3[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "start_logit (Dense)             (None, 384, 1)       768         tf_bert_model[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "end_logit (Dense)               (None, 384, 1)       768         tf_bert_model[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 384)          0           start_logit[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 384)          0           end_logit[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 384)          0           flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 384)          0           flatten_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 109,483,776\n",
            "Trainable params: 109,483,776\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btTMsiy4XbiF"
      },
      "source": [
        "## Create evaluation Callback\n",
        "\n",
        "This callback will compute the exact match score using the validation data\n",
        "after every epoch.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QorV5BS3XbiF"
      },
      "source": [
        "\n",
        "def normalize_text(text):\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuations\n",
        "    exclude = set(string.punctuation)\n",
        "    text = \"\".join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    # Remove articles\n",
        "    regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
        "    text = re.sub(regex, \" \", text)\n",
        "\n",
        "    # Remove extra white space\n",
        "    text = \" \".join(text.split())\n",
        "    return text\n",
        "\n",
        "\n",
        "class ExactMatch(keras.callbacks.Callback):\n",
        "    \"\"\"\n",
        "    Each `SquadExample` object contains the character level offsets for each token\n",
        "    in its input paragraph. We use them to get back the span of text corresponding\n",
        "    to the tokens between our predicted start and end tokens.\n",
        "    All the ground-truth answers are also present in each `SquadExample` object.\n",
        "    We calculate the percentage of data points where the span of text obtained\n",
        "    from model predictions matches one of the ground-truth answers.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, x_eval, y_eval):\n",
        "        self.x_eval = x_eval\n",
        "        self.y_eval = y_eval\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        pred_start, pred_end = self.model.predict(self.x_eval)\n",
        "\n",
        "\n",
        "        print( \"====\",self.x_eval,\"====\")\n",
        "\n",
        "        count = 0\n",
        "        eval_examples_no_skip = [_ for _ in eval_squad_examples if _.skip == False]\n",
        "        for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n",
        "            squad_eg = eval_examples_no_skip[idx]\n",
        "            offsets = squad_eg.context_token_to_char\n",
        "            start = np.argmax(start)\n",
        "            end = np.argmax(end)\n",
        "            if start >= len(offsets):\n",
        "                continue\n",
        "            pred_char_start = offsets[start][0]\n",
        "            if end < len(offsets):\n",
        "                pred_char_end = offsets[end][1]\n",
        "                pred_ans = squad_eg.context[pred_char_start:pred_char_end]\n",
        "            else:\n",
        "                pred_ans = squad_eg.context[pred_char_start:]\n",
        "\n",
        "            normalized_pred_ans = normalize_text(pred_ans)\n",
        "            normalized_true_ans = [normalize_text(_) for _ in squad_eg.all_answers]\n",
        "            if normalized_pred_ans in normalized_true_ans:\n",
        "                count += 1\n",
        "        acc = count / len(self.y_eval[0])\n",
        "        print(f\"\\nepoch={epoch+1}, exact match score={acc:.2f}\")\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7766n0WRXbiF"
      },
      "source": [
        "## Train and Evaluate\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LzeP2c3XbiF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab5fcf83-e9ad-46ee-c8b2-84dddeb70f21"
      },
      "source": [
        "exact_match_callback = ExactMatch(x_eval, y_eval)\n",
        "model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=1,  # For demonstration, 3 epochs are recommended\n",
        "    verbose=2,\n",
        "    batch_size=64,\n",
        "    callbacks=[exact_match_callback],\n",
        ")\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1346/1346 - 229s - loss: 1.5565 - activation_4_loss: 0.8283 - activation_5_loss: 0.7283\n",
            "==== [array([[ 101, 3565, 4605, ...,    0,    0,    0],\n",
            "       [ 101, 3565, 4605, ...,    0,    0,    0],\n",
            "       [ 101, 3565, 4605, ...,    0,    0,    0],\n",
            "       ...,\n",
            "       [ 101, 1996, 9044, ...,    0,    0,    0],\n",
            "       [ 101, 1996, 9044, ...,    0,    0,    0],\n",
            "       [ 101, 1996, 9044, ...,    0,    0,    0]]), array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]]), array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])] ====\n",
            "\n",
            "epoch=1, exact match score=0.79\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd4dea4dcc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-BkaHQbKSDT",
        "outputId": "986dc3b5-21f6-479a-af72-1ef99a1ce951",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(create_inputs_targets(eval_squad_examples))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHvmvHbzMKFJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_knwL-wCK5zc"
      },
      "source": [
        "pred_start, pred_end = model.predict(create_inputs_targets( eval_squad_examples)[0] )"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqY4dTNj8Xoo"
      },
      "source": [
        "answers = {}\n",
        "eval_examples_no_skip = [_ for _ in eval_squad_examples if _.skip == False]\n",
        "for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n",
        "    squad_eg = eval_examples_no_skip[idx]\n",
        "    offsets = squad_eg.context_token_to_char\n",
        "    start = np.argmax(start)\n",
        "    end = np.argmax(end)\n",
        "    if start >= len(offsets):\n",
        "        continue\n",
        "    pred_char_start = offsets[start][0]\n",
        "    if end < len(offsets):\n",
        "        pred_char_end = offsets[end][1]\n",
        "        pred_ans = squad_eg.context[pred_char_start:pred_char_end]\n",
        "    else:\n",
        "        pred_ans = squad_eg.context[pred_char_start:]\n",
        "    \n",
        "    # 答え一覧に入れる\n",
        "    answers[idx] = pred_ans"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVMPVK84_w57"
      },
      "source": [
        "def showQA(idx):\n",
        "    print(\"問題 : \",eval_examples_no_skip[idx].question)\n",
        "    print(\"予測解 : \",answers[idx])\n",
        "    print(\"真の正解 : \",eval_examples_no_skip[idx].answer_text )\n",
        "    return eval_examples_no_skip[idx].question"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKJL1z9MY7mB"
      },
      "source": [
        "def returnQ(idx):\n",
        "    return eval_examples_no_skip[idx].question"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYCFyzGXB1xs"
      },
      "source": [
        "0~10331 までの数字を入れると、問題とその予測の結果の答えが帰ってくる"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPwaUC3J-5PG",
        "outputId": "bc77d419-3c28-444e-db93-84a4d9fd50ec"
      },
      "source": [
        "showQA(104)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "問題 :  What other two famous performers were part of the Super Bowl 50 halftime?\n",
            "予測解 :  Beyoncé and Bruno Mars\n",
            "真の正解 :  Beyoncé and Bruno Mars\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8KHjmwrCADB",
        "outputId": "acc83566-1f3f-469a-ffb5-733a35619749"
      },
      "source": [
        "showQA(4)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "問題 :  What color was used to emphasize the 50th anniversary of the Super Bowl?\n",
            "予測解 :  gold\n",
            "真の正解 :  gold\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8enGOYY_CMmB"
      },
      "source": [
        "# import re\n",
        "# for i in range(len(eval_examples_no_skip)):\n",
        "#     question = returnQ(i)\n",
        "\n",
        "#     if re.match(r\"What was Warsaw\", question):\n",
        "#         print(i)"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7XX7Qk6ZF-M"
      },
      "source": [
        "# showQA(787)"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMpTPB8PCgAI"
      },
      "source": [
        "# my_data = {\"data\":[{\"paragraphs\" :[{\"context\":\"Demographically, it was the most diverse city in Poland, with significant numbers of foreign-born inhabitants. In addition to the Polish majority, there was a significant Jewish minority in Warsaw. According to Russian census of 1897, out of the total population of 638,000, Jews constituted 219,000 (around 34% percent). Warsaw's prewar Jewish population of more than 350,000 constituted about 30 percent of the city's total population. In 1933, out of 1,178,914 inhabitants 833,500 were of Polish mother tongue. World War II changed the demographics of the city, and to this day there is much less ethnic diversity than in the previous 300 years of Warsaw's history. Most of the modern day population growth is based on internal migration and urbanisation.\",\\\n",
        "#                                  'qas':[{'answers': [{'answer_start': 41, 'text': 'city'},{'answer_start': 0, 'text': 'Demographically'},{'answer_start': 41, 'text': 'city'}],'id': '57337ea24776f41900660bd0','question': 'What was Warsaw the most diverse of in Poland?'}]\n",
        "\n",
        "# }] }] }"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzbxQ_wGSYp8"
      },
      "source": [
        "context = \"Meitoma is 31 years old. He's from Osaka Neyagawa City.He graduate Kyoto University. Kyoto University is as high level as Tokyo University\"\n",
        "question = \"Which college Meitoma graduate\"\n",
        "\n",
        "my_data = {\"data\":[{\"paragraphs\" :[{\"context\":context,\n",
        "                                 'qas':[{ \"answers\" :[{\"answer_start\":0, \"text\":\"aaaa\"}] ,'question': question}]\n",
        "}] }] }"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2up22w5ZQcXO"
      },
      "source": [
        "my_example = create_squad_examples(my_data)\n",
        "\n",
        "my_x , my_y =  create_inputs_targets( my_example )\n",
        "\n",
        "pred_start, pred_end  = model.predict(  my_x )"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_r9UJ_6Q9ln",
        "outputId": "ffc7226f-82ce-4daa-be99-a79f873de80e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "offsets = my_example[0].context_token_to_char\n",
        "\n",
        "x_eval, y_eval = create_inputs_targets(eval_squad_examples)\n",
        "\n",
        "start = np.argmax(pred_start[0])\n",
        "end = np.argmax(pred_end[0])\n",
        "\n",
        "pred_char_start = offsets[start][0]\n",
        "pred_char_end = offsets[end][1]\n",
        "\n",
        "pred_ans = my_example[0].context[pred_char_start : pred_char_end]\n",
        "print(pred_ans)"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Kyoto University\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}