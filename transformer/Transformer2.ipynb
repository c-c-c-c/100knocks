{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xlRy0GRvaxKg"
   },
   "source": [
    "# <center>Transformer from scratch２：モデルの学習</center>\n",
    "## <center>最終更新日(超未完成版)：2021.5/8</center>\n",
    "\n",
    "自己アテンションに基づくTransformerアーキテクチャをtf.Kerasで実装するノートブックです。同じような内容は\n",
    "\n",
    "https://www.tensorflow.org/tutorials/text/transformer?hl=ja\n",
    "\n",
    "にあります。また、スクラッチ実装をしなくても多くのライブラリでは簡単にSelf-Attention層が利用できます。しかしちゃんとTransformerを理解したりTransformerベースのモデルを開発したいなら、スクラッチ実装の経験がないといけないと思いますので、勉強していきましょう！\n",
    "\n",
    "[参考文献]\n",
    "\n",
    "https://arxiv.org/pdf/1706.03762.pdf\n",
    "\n",
    "このノートブックでは学習の実装を扱います。\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "前回のノートブックでモデルの準備ができましたので、いよいよ訓練の実装を行い、本物のデータで訓練してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ga33C6o_bAlV"
   },
   "source": [
    "# 0. 準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k1cmoYucbbO3"
   },
   "source": [
    "## 0.1 実装したモジュール等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Rp7vM1H6avAj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# configuration\n",
    "source_vocab_size = 100000\n",
    "max_seq_len = 100\n",
    "d_model = 512\n",
    "do_rate = 0.2\n",
    "d_ff = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZNnoQj5RcvKU"
   },
   "source": [
    "utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "foFPvsLpbHo5"
   },
   "outputs": [],
   "source": [
    "def positional_encoding_function(t, i, d):\n",
    "    theta = t / 10000**(2*(i//2)/d) + np.pi/2 * (i%2)\n",
    "    return np.sin(theta)\n",
    "\n",
    "def positional_encoding(T, d):\n",
    "    dd, TT = np.meshgrid(np.arange(d), np.arange(T))\n",
    "    encoding = positional_encoding_function(TT, dd, d)\n",
    "    encoding = encoding[np.newaxis,:,:]\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3oogxtCEcmvB"
   },
   "source": [
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mrv4Soovb0Sk"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, max_seq_len, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.pos_encoding = positional_encoding(max_seq_len, d_model)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_dtype = inputs.dtype\n",
    "        pos_encoding = self.pos_encoding\n",
    "        pos_encoding = tf.cast(pos_encoding, dtype=input_dtype)\n",
    "        inputs *= tf.math.sqrt(tf.cast(self.d_model, dtype=input_dtype))\n",
    "\n",
    "        return inputs + pos_encoding\n",
    "\n",
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    def __init__(self, n_heads, d_model):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert self.d_model % self.n_heads == 0, 'n_headsはd_modelの因数'\n",
    "\n",
    "        self.d = self.d_model // self.n_heads\n",
    "\n",
    "        self.dense_q = layers.Dense(d_model, use_bias=False)\n",
    "        self.dense_k = layers.Dense(d_model, use_bias=False)\n",
    "        self.dense_v = layers.Dense(d_model, use_bias=False)\n",
    "        self.dense_o = layers.Dense(d_model, use_bias=False)\n",
    "\n",
    "    def call(self, x_q, x_k, x_v, mask):\n",
    "        \n",
    "        q = self.dense_q(x_q)\n",
    "        k = self.dense_k(x_k)\n",
    "        v = self.dense_v(x_v)\n",
    "\n",
    "        max_seq_len = tf.shape(k)[1]\n",
    "        \n",
    "        q = self.split_to_heads(q, max_seq_len)\n",
    "        k = self.split_to_heads(k, max_seq_len)\n",
    "        v = self.split_to_heads(v, max_seq_len)\n",
    "\n",
    "        logit = tf.matmul(q, k, transpose_b=True)\n",
    "\n",
    "        d_k = tf.shape(k)[-1]\n",
    "        k_dtype = k.dtype\n",
    "        scale = 1/tf.math.sqrt(tf.cast(d_k, k_dtype))\n",
    "        logit *= scale\n",
    "\n",
    "        if mask is not None:\n",
    "            logit += mask*k_dtype.min\n",
    "        \n",
    "        attention_weight = tf.nn.softmax(logit, axis=-1)\n",
    "        attention_output = tf.einsum('nhst,nhtd->nshd', attention_weight, v)\n",
    "        attention_output = tf.reshape(attention_output, (-1, max_seq_len, self.n_heads*self.d))\n",
    "        attention_output = self.dense_o(attention_output)\n",
    "\n",
    "        return attention_output, attention_weight\n",
    "\n",
    "    def split_to_heads(self, x, max_seq_len):\n",
    "        x = tf.reshape(x, (-1, max_seq_len, self.n_heads, self.d))\n",
    "        x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        return x\n",
    "\n",
    "class MHSAModule(layers.Layer):\n",
    "    def __init__(self, source_vocab_size, max_seq_len, d_model, n_heads, do_rate):\n",
    "        super(MHSAModule, self).__init__()\n",
    "        self.mhsa = MultiHeadSelfAttention(n_heads, d_model)\n",
    "        self.dropout = layers.Dropout(do_rate)\n",
    "        self.add = layers.Add()\n",
    "        self.norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs, training, mask):\n",
    "        x, att = self.mhsa(inputs, inputs, inputs, mask)\n",
    "        x = self.dropout(x, training=training)\n",
    "        x = self.add([x,inputs])\n",
    "        x = self.norm(x)\n",
    "        return x, att\n",
    "\n",
    "class PointWiseFeedForwardModule(layers.Layer):\n",
    "    def __init__(self, d_model, d_ff, do_rate):\n",
    "        super(PointWiseFeedForwardModule, self).__init__()\n",
    "        self.pwff_1 = layers.Dense(d_ff, activation='relu')\n",
    "        self.pwff_2 = layers.Dense(d_model)\n",
    "        self.dropout = layers.Dropout(do_rate)\n",
    "        self.add = layers.Add()\n",
    "        self.norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        x = self.pwff_1(inputs)\n",
    "        x = self.pwff_2(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        x = self.add([x,inputs])\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "class EncoderModule(layers.Layer):\n",
    "    def __init__(self, source_vocab_size, max_seq_len, d_model, n_heads, do_rate):\n",
    "        super(EncoderModule, self).__init__()\n",
    "        self.mhsa = MHSAModule(source_vocab_size, max_seq_len, d_model, n_heads, do_rate)\n",
    "        self.pwff = PointWiseFeedForwardModule(d_model, d_ff, do_rate)\n",
    "\n",
    "    def call(self, inputs, training, mask):\n",
    "        x, att = self.mhsa(inputs, mask, training)\n",
    "        x = self.pwff(x, training)\n",
    "        return x, att\n",
    "\n",
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, n_layers, source_vocab_size, max_seq_len, d_model, n_heads, do_rate):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.source_vocab_size = source_vocab_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.do_rate = do_rate\n",
    "        self.do = layers.Dropout(do_rate)\n",
    "        self.embedding = layers.Embedding(source_vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(max_seq_len, d_model)\n",
    "        self.mhsa_modules = [EncoderModule(source_vocab_size, max_seq_len, d_model, n_heads, do_rate) for _ in range(n_layers)]\n",
    "\n",
    "    def call(self, inputs, training, mask_enc):\n",
    "        x = self.embedding(inputs)\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.do(x)\n",
    "        attention_weights = []\n",
    "        for module in self.mhsa_modules:\n",
    "            x, att = module(x, training, mask_enc)\n",
    "            attention_weights.append(att)\n",
    "        return x, attention_weights\n",
    "\n",
    "class MHSAModuleDec(layers.Layer):\n",
    "    def __init__(self, source_vocab_size, max_seq_len, d_model, n_heads, do_rate):\n",
    "        super(MHSAModuleDec, self).__init__()\n",
    "        self.mhsa = MultiHeadSelfAttention(n_heads, d_model)\n",
    "        self.dropout = layers.Dropout(do_rate)\n",
    "        self.add = layers.Add()\n",
    "        self.norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs, context, training, mask):\n",
    "        x, att = self.mhsa(inputs, context, context, mask)\n",
    "        x = self.dropout(x, training=training)\n",
    "        x = self.add([x,inputs])\n",
    "        x = self.norm(x)\n",
    "        return x, att\n",
    "\n",
    "\n",
    "    \n",
    "class DecoderModule(layers.Layer):\n",
    "    def __init__(self, source_vocab_size, max_seq_len, d_model, n_heads, do_rate):\n",
    "        super(DecoderModule, self).__init__()\n",
    "        self.mhsa1 = MHSAModule(source_vocab_size, max_seq_len, d_model, n_heads, do_rate)\n",
    "        self.mhsa2 = MHSAModuleDec(source_vocab_size, max_seq_len, d_model, n_heads, do_rate)\n",
    "        self.pwff1 = PointWiseFeedForwardModule(d_model, d_ff, do_rate)\n",
    "        self.pwff2 = PointWiseFeedForwardModule(d_model, d_ff, do_rate)\n",
    "    def call(self, inputs, enc_outputs, training, mask, mask_look_ahead):\n",
    "        x, att1 = self.mhsa1(inputs, training, mask)\n",
    "        # 修正！！！\n",
    "        x = self.pwff1(x, training)\n",
    "        x, att2 = self.mhsa2(x, enc_outputs, training, mask_look_ahead)\n",
    "        x = self.pwff2(x, training)\n",
    "        return x, att1, att2\n",
    "    \n",
    "    \n",
    "\n",
    "class Decoder(layers.Layer):\n",
    "    def __init__(self, n_layers, target_vocab_size, max_seq_len, d_model, n_heads, do_rate):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.target_vocab_size = target_vocab_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.do_rate = do_rate\n",
    "        self.do = layers.Dropout(do_rate)\n",
    "        self.embedding = layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(max_seq_len, d_model)\n",
    "        self.mhsa_modules = [DecoderModule(source_vocab_size, max_seq_len, d_model, n_heads, do_rate) for _ in range(n_layers)]\n",
    "\n",
    "    def call(self, inputs, enc_outputs, training, mask_dec, mask_look_ahead):\n",
    "        x = self.embedding(inputs)\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.do(x)\n",
    "        attention_weights = []\n",
    "        for module in self.mhsa_modules:\n",
    "            x, att1, att2 = module(x, enc_outputs, training, mask_dec, mask_look_ahead)\n",
    "            attention_weights += [[att1, att2]]\n",
    "        return x, attention_weights\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2H03q1IYcoqj"
   },
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Vi1XeG0lceG6"
   },
   "outputs": [],
   "source": [
    "class Transformer(layers.Layer):\n",
    "    def __init__(self, n_layers, source_vocab_size, target_vocab_size, max_seq_len, d_model, n_heads, do_rate):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.target_vocab_size = target_vocab_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.do_rate = do_rate\n",
    "        self.encoder = Encoder(n_layers, source_vocab_size, max_seq_len, d_model, n_heads, do_rate)\n",
    "        self.decoder = Decoder(n_layers, target_vocab_size, max_seq_len, d_model, n_heads, do_rate)\n",
    "        self.classification = layers.Dense(target_vocab_size-1)\n",
    "\n",
    "    def call(self, enc_inputs, dec_inputs, mask_enc, mask_dec, mask_look_ahead, training=False):\n",
    "\n",
    "        enc_outputs, att_enc = self.encoder(enc_inputs, training=training, mask_enc=mask_enc)\n",
    "        dec_outputs, att_dec = self.decoder(dec_inputs, enc_outputs, training=training, mask_dec=mask_dec, mask_look_ahead=mask_look_ahead)\n",
    "        y = self.classification(dec_outputs)\n",
    "\n",
    "        return y, att_enc + att_dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "STHqif2IbgCb"
   },
   "source": [
    "## 0.2 データの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 49852,
     "status": "ok",
     "timestamp": 1620807249402,
     "user": {
      "displayName": "MASATO TAKI",
      "photoUrl": "",
      "userId": "06408555804759539719"
     },
     "user_tz": -540
    },
    "id": "UzlW1xYebtGo",
    "outputId": "3c965c37-d1c4-486d-eb54-6c3767ee9543"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  aptitude-common libcgi-fast-perl libcgi-pm-perl libclass-accessor-perl\n",
      "  libcwidget3v5 libfcgi-perl libio-string-perl libparse-debianchangelog-perl\n",
      "  libsigc++-2.0-0v5 libsub-name-perl libxapian30\n",
      "Suggested packages:\n",
      "  aptitude-doc-en | aptitude-doc apt-xapian-index debtags tasksel\n",
      "  libcwidget-dev libhtml-template-perl libxml-simple-perl xapian-tools\n",
      "The following NEW packages will be installed:\n",
      "  aptitude aptitude-common libcgi-fast-perl libcgi-pm-perl\n",
      "  libclass-accessor-perl libcwidget3v5 libfcgi-perl libio-string-perl\n",
      "  libparse-debianchangelog-perl libsigc++-2.0-0v5 libsub-name-perl libxapian30\n",
      "0 upgraded, 12 newly installed, 0 to remove and 50 not upgraded.\n",
      "Need to get 3533 kB of archives.\n",
      "After this operation, 14.5 MB of additional disk space will be used.\n",
      "Do you want to continue? [Y/n] "
     ]
    }
   ],
   "source": [
    "!apt install aptitude\n",
    "!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
    "!pip install mecab-python3==0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2994,
     "status": "ok",
     "timestamp": 1620807797052,
     "user": {
      "displayName": "MASATO TAKI",
      "photoUrl": "",
      "userId": "06408555804759539719"
     },
     "user_tz": -540
    },
    "id": "c0zFqWcNbjPw",
    "outputId": "725f5322-99dc-48bf-c09c-1fc4219a89ef"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import MeCab\n",
    "import re\n",
    "#from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "#from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "num_samples = 10000  # 訓練に使うサンプルの数。この中の１割をvalに使う\n",
    "\n",
    "text_path = '/content/drive/MyDrive/ml_datasets/jpn-eng/jpn.txt'\n",
    "\n",
    "with open(text_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "\n",
    "#input_text_sp = re.split('\\s', 'No way!'.replace('!', ''))\n",
    "#input_text_sp.append('!')\n",
    "\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "\n",
    "tagger = MeCab.Tagger('-Owakati')\n",
    "\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text, _ = line.split('\\t')\n",
    "\n",
    "    last_char = input_text[-1]\n",
    "    if last_char in {'.', '!', '?'}:\n",
    "        input_text_replaced = input_text.replace(last_char, '')\n",
    "        input_text_sp = re.split('\\s', input_text_replaced)\n",
    "        input_text_sp.append(last_char)\n",
    "    else:\n",
    "        input_text_sp = re.split('\\s', input_text)\n",
    "\n",
    "    for word in input_text_sp:\n",
    "        if word not in input_characters:\n",
    "            input_characters.add(word)\n",
    "    input_texts.append(input_text_sp)\n",
    "    \n",
    "    # '\\t'を出力文の開始記号SOS、'\\n'を終了記号EOSに使う\n",
    "    result = tagger.parse(target_text)\n",
    "    wakachi = result.split()\n",
    "    wakachi = ['\\t'] + wakachi + ['\\n']\n",
    "    for word in wakachi:\n",
    "        if word not in target_characters:\n",
    "            target_characters.add(word)\n",
    "    target_texts.append(wakachi)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "\n",
    "print('num of input characters w/out pad:', len(input_characters))\n",
    "print('num of output characters w/out pad:', len(target_characters))\n",
    "\n",
    "\n",
    "# padding記号も含めておく\n",
    "num_encoder_tokens = len(input_characters) + 1\n",
    "num_decoder_tokens = len(target_characters) + 1\n",
    "\n",
    "# ここはハイパーパラメータ化\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "max_seq_len = max(max_encoder_seq_length, max_decoder_seq_length)\n",
    "\n",
    "# padding記号を0にする\n",
    "input_token_index = dict(\n",
    "    [(char, i+1) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i+1) for i, char in enumerate(target_characters)])\n",
    " \n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_seq_len),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_seq_len),\n",
    "    dtype='float32')\n",
    "\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_seq_len, num_decoder_tokens-1),\n",
    "    dtype='float32')\n",
    " \n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t] = input_token_index[char]\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t] = target_token_index[char]\n",
    "        if t > 0:\n",
    "            # 次の文字予測なので、targetは2文字目から始める\n",
    "            # targetはpad記号を含まない(num_decoder_tokens-1)文字に対するone-hot\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]-1] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yE47OHaYpfuU"
   },
   "source": [
    "Padding記号も含めた後では"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1217,
     "status": "ok",
     "timestamp": 1620807801381,
     "user": {
      "displayName": "MASATO TAKI",
      "photoUrl": "",
      "userId": "06408555804759539719"
     },
     "user_tz": -540
    },
    "id": "odVBQGmBpXnA",
    "outputId": "c1e94c10-9b9a-4ca0-a6df-36b39028861e"
   },
   "outputs": [],
   "source": [
    "print('num_encoder_tokens', num_encoder_tokens)\n",
    "print('num_decoder_tokens', num_decoder_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y7PWmQRuQSgd"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1068,
     "status": "ok",
     "timestamp": 1620807878524,
     "user": {
      "displayName": "MASATO TAKI",
      "photoUrl": "",
      "userId": "06408555804759539719"
     },
     "user_tz": -540
    },
    "id": "gS2ygw_teB9j",
    "outputId": "2105e358-69d7-4b35-c394-92664151bd7b"
   },
   "outputs": [],
   "source": [
    "encoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 996,
     "status": "ok",
     "timestamp": 1620807879245,
     "user": {
      "displayName": "MASATO TAKI",
      "photoUrl": "",
      "userId": "06408555804759539719"
     },
     "user_tz": -540
    },
    "id": "5qOyTpe_eP2Q",
    "outputId": "b077fe90-af8c-4d10-9fcd-47036f1d9dd1"
   },
   "outputs": [],
   "source": [
    "decoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 425,
     "status": "ok",
     "timestamp": 1620807879642,
     "user": {
      "displayName": "MASATO TAKI",
      "photoUrl": "",
      "userId": "06408555804759539719"
     },
     "user_tz": -540
    },
    "id": "IlOKyK_reSI1",
    "outputId": "0b009771-66b0-46b5-a5e2-a570b0b22c61"
   },
   "outputs": [],
   "source": [
    "decoder_target_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qulTiDwWvDO9"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QxYEtDXBr3ZW"
   },
   "outputs": [],
   "source": [
    "def custom_loss(y_true,y_pred):\n",
    "    loss = -tf.reduce_sum(y_true * tf.math.log(y_pred+1e-16), axis=-1)\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 905,
     "status": "ok",
     "timestamp": 1620812743744,
     "user": {
      "displayName": "MASATO TAKI",
      "photoUrl": "",
      "userId": "06408555804759539719"
     },
     "user_tz": -540
    },
    "id": "hpP8okRCu8SU",
    "outputId": "ecb3f8b1-a608-4f6e-928b-05503254ebdb"
   },
   "outputs": [],
   "source": [
    "custom_loss(decoder_target_data[:10,:,:],decoder_target_data[:10,:,:]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1521,
     "status": "ok",
     "timestamp": 1620812744950,
     "user": {
      "displayName": "MASATO TAKI",
      "photoUrl": "",
      "userId": "06408555804759539719"
     },
     "user_tz": -540
    },
    "id": "DMJLaLO0uLR_",
    "outputId": "53fd1b59-f905-4701-e3bc-0e9462e0cd07"
   },
   "outputs": [],
   "source": [
    "custom_loss(decoder_target_data[:10,:,:],decoder_target_data[8:18,:,:]).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQnsljTTc7Yf"
   },
   "source": [
    "# 1. 訓練プロセスの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TegBO2Ppd3uo"
   },
   "source": [
    "## 1-1. マスクについて"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zkuu2eoUc-33"
   },
   "outputs": [],
   "source": [
    "encoder_mask_data = np.where(encoder_input_data!=0, 0, 1)\n",
    "encoder_mask_data = encoder_mask_data.astype('float32')\n",
    "encoder_mask_data = encoder_mask_data[:, np.newaxis, np.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1061,
     "status": "ok",
     "timestamp": 1620809723144,
     "user": {
      "displayName": "MASATO TAKI",
      "photoUrl": "",
      "userId": "06408555804759539719"
     },
     "user_tz": -540
    },
    "id": "gCHvF8exeixN",
    "outputId": "1eb30bf1-a0c8-430f-ab15-5c835feb263b"
   },
   "outputs": [],
   "source": [
    "encoder_mask_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1053,
     "status": "ok",
     "timestamp": 1620809726904,
     "user": {
      "displayName": "MASATO TAKI",
      "photoUrl": "",
      "userId": "06408555804759539719"
     },
     "user_tz": -540
    },
    "id": "cUCzcqSce7K8",
    "outputId": "5c859cb1-fd04-40cf-c4ce-a5a8d14177bb"
   },
   "outputs": [],
   "source": [
    "encoder_input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FYXPjhjve79s"
   },
   "outputs": [],
   "source": [
    "decoder_mask_data = np.where(decoder_input_data!=0, 0, 1)\n",
    "decoder_mask_data = decoder_mask_data.astype('float32')\n",
    "decoder_mask_data = decoder_mask_data[:, np.newaxis, np.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 771,
     "status": "ok",
     "timestamp": 1620809738024,
     "user": {
      "displayName": "MASATO TAKI",
      "photoUrl": "",
      "userId": "06408555804759539719"
     },
     "user_tz": -540
    },
    "id": "deI4quM8fLk2",
    "outputId": "f5049829-c79c-4813-b96d-a88e586e3dfb"
   },
   "outputs": [],
   "source": [
    "decoder_mask_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zPiHcgLVvm6I"
   },
   "outputs": [],
   "source": [
    "look_ahead = 1.-np.tril(np.ones((max_seq_len,max_seq_len)))\n",
    "look_ahead = look_ahead[np.newaxis, np.newaxis, :, :]\n",
    "\n",
    "decoder_mask_look_ahead_data = np.clip(decoder_mask_data + look_ahead, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 943,
     "status": "ok",
     "timestamp": 1620809754907,
     "user": {
      "displayName": "MASATO TAKI",
      "photoUrl": "",
      "userId": "06408555804759539719"
     },
     "user_tz": -540
    },
    "id": "9UDdYjSow0kW",
    "outputId": "7f02f7ed-9646-434b-a256-1d2fbaf3f4fa"
   },
   "outputs": [],
   "source": [
    "decoder_mask_look_ahead_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 772,
     "status": "ok",
     "timestamp": 1620799154035,
     "user": {
      "displayName": "MASATO TAKI",
      "photoUrl": "",
      "userId": "06408555804759539719"
     },
     "user_tz": -540
    },
    "id": "ggdaIDqhfNvJ",
    "outputId": "11f750ef-b2ca-492d-aa0a-5acc1824e9db"
   },
   "outputs": [],
   "source": [
    "decoder_input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2154,
     "status": "ok",
     "timestamp": 1620813339862,
     "user": {
      "displayName": "MASATO TAKI",
      "photoUrl": "",
      "userId": "06408555804759539719"
     },
     "user_tz": -540
    },
    "id": "7l0i8aLYfOjh",
    "outputId": "6999bb46-d527-4c03-99a6-38cb31b4974e"
   },
   "outputs": [],
   "source": [
    "n_heads = 8\n",
    "n_layers = 2\n",
    "d_ff = 1024#2048\n",
    "d_model = 256\n",
    "do_rate = 0.2\n",
    "\n",
    "encoder_inputs = layers.Input(shape=(max_seq_len,), name='encoder_input')\n",
    "decoder_inputs = layers.Input(shape=(max_seq_len,), name='decoder_input')\n",
    "\n",
    "encoder_mask = layers.Input(shape=(1,1,max_seq_len))\n",
    "#decoder_mask = layers.Input(shape=(1,1,max_seq_len))\n",
    "decoder_look_ahead_mask = layers.Input(shape=(1,max_seq_len,max_seq_len))\n",
    "\n",
    "transformer = Transformer(n_layers, num_encoder_tokens, num_decoder_tokens, max_seq_len, d_model, n_heads, do_rate)\n",
    "y = transformer(encoder_inputs, decoder_inputs, mask_enc=encoder_mask, mask_dec=encoder_mask, mask_look_ahead=decoder_look_ahead_mask, training=True)\n",
    "\n",
    "transformer_model = tf.keras.models.Model(inputs=[encoder_inputs, decoder_inputs, encoder_mask, decoder_look_ahead_mask], outputs=y[0], name='trasnformer')\n",
    "\n",
    "transformer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zvehgftlf3jH"
   },
   "outputs": [],
   "source": [
    "transformer_model.compile(optimizer='rmsprop', loss=custom_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TBWadFyrP8Na"
   },
   "source": [
    "上の実装のtargetの3982次元「one-hot」は、padding記号に対しては、全てゼロなので、この無意味なpadding記号にはlossを計算しないようにしたcrossentropy関数である`custom_loss`を使うことにした。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 727554,
     "status": "ok",
     "timestamp": 1620814069326,
     "user": {
      "displayName": "MASATO TAKI",
      "photoUrl": "",
      "userId": "06408555804759539719"
     },
     "user_tz": -540
    },
    "id": "BXShVshIyGPe",
    "outputId": "b1e6d677-2e6a-41ae-a8c6-677d640356b4"
   },
   "outputs": [],
   "source": [
    "transformer_model.fit(x=[encoder_input_data, decoder_input_data, encoder_mask_data, decoder_mask_look_ahead_data], y=decoder_target_data, epochs=16, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4CNBVIO5C-Vk"
   },
   "outputs": [],
   "source": [
    "n_heads = 8\n",
    "n_layers = 2\n",
    "d_ff = 512\n",
    "d_model = 128\n",
    "do_rate = 0.2\n",
    "\n",
    "encoder_inputs = layers.Input(shape=(max_seq_len,), name='encoder_input')\n",
    "decoder_inputs = layers.Input(shape=(max_seq_len,), name='decoder_input')\n",
    "\n",
    "encoder_mask = layers.Input(shape=(1,1,max_seq_len))\n",
    "decoder_look_ahead_mask = layers.Input(shape=(1,max_seq_len,max_seq_len))\n",
    "\n",
    "transformer = Transformer(n_layers, num_encoder_tokens, num_decoder_tokens, max_seq_len, d_model, n_heads, do_rate)\n",
    "y = transformer(encoder_inputs, decoder_inputs, mask_enc=encoder_mask, mask_dec=encoder_mask, mask_look_ahead=decoder_look_ahead_mask, training=True)\n",
    "\n",
    "transformer_model = tf.keras.models.Model(inputs=[encoder_inputs, decoder_inputs, encoder_mask, decoder_look_ahead_mask], outputs=y[0], name='trasnformer')\n",
    "\n",
    "transformer_model.compile(optimizer='rmsprop', loss=custom_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S_8XvE4wDJ4H",
    "outputId": "8e1c39fc-16d8-4789-bcbb-fc386356614a"
   },
   "outputs": [],
   "source": [
    "transformer_model.fit(x=[encoder_input_data, decoder_input_data, encoder_mask_data, decoder_mask_look_ahead_data], \n",
    "                      y=decoder_target_data, epochs=16, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Cs6g_kj16su"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, max_seq_len, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.pos_encoding = positional_encoding(max_seq_len, d_model)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_dtype = inputs.dtype\n",
    "        pos_encoding = self.pos_encoding\n",
    "        pos_encoding = tf.cast(pos_encoding, dtype=input_dtype)\n",
    "        inputs *= tf.math.sqrt(tf.cast(self.d_model, dtype=input_dtype))\n",
    "\n",
    "        return inputs + pos_encoding\n",
    "\n",
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    def __init__(self, n_heads, d_model):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert self.d_model % self.n_heads == 0, 'n_headsはd_modelの因数'\n",
    "\n",
    "        self.d = self.d_model // self.n_heads\n",
    "\n",
    "        self.dense_q = layers.Dense(d_model, use_bias=False)\n",
    "        self.dense_k = layers.Dense(d_model, use_bias=False)\n",
    "        self.dense_v = layers.Dense(d_model, use_bias=False)\n",
    "        self.dense_o = layers.Dense(d_model, use_bias=False)\n",
    "\n",
    "    def call(self, x_q, x_k, x_v, mask):\n",
    "        \n",
    "        q = self.dense_q(x_q)\n",
    "        k = self.dense_k(x_k)\n",
    "        v = self.dense_v(x_v)\n",
    "\n",
    "        max_seq_len = tf.shape(k)[1]\n",
    "        \n",
    "        q = self.split_to_heads(q, max_seq_len)\n",
    "        k = self.split_to_heads(k, max_seq_len)\n",
    "        v = self.split_to_heads(v, max_seq_len)\n",
    "\n",
    "        logit = tf.matmul(q, k, transpose_b=True)\n",
    "\n",
    "        d_k = tf.shape(k)[-1]\n",
    "        k_dtype = k.dtype\n",
    "        scale = 1/tf.math.sqrt(tf.cast(d_k, k_dtype))\n",
    "        logit *= scale\n",
    "\n",
    "        if mask is not None:\n",
    "            logit += mask*k_dtype.min\n",
    "        \n",
    "        attention_weight = tf.nn.softmax(logit, axis=-1)\n",
    "        attention_output = tf.einsum('nhst,nhtd->nshd', attention_weight, v)\n",
    "        attention_output = tf.reshape(attention_output, (-1, max_seq_len, self.n_heads*self.d))\n",
    "        attention_output = self.dense_o(attention_output)\n",
    "\n",
    "        return attention_output, attention_weight\n",
    "\n",
    "    def split_to_heads(self, x, max_seq_len):\n",
    "        x = tf.reshape(x, (-1, max_seq_len, self.n_heads, self.d))\n",
    "        x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        return x\n",
    "\n",
    "class MHSAModule(layers.Layer):\n",
    "    def __init__(self, source_vocab_size, max_seq_len, d_model, n_heads, do_rate):\n",
    "        super(MHSAModule, self).__init__()\n",
    "        self.mhsa = MultiHeadSelfAttention(n_heads, d_model)\n",
    "        self.dropout = layers.Dropout(do_rate)\n",
    "        self.add = layers.Add()\n",
    "        self.norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs, mask, training=False):\n",
    "        x, att = self.mhsa(inputs, inputs, inputs, mask)\n",
    "        x = self.dropout(x, training=training)\n",
    "        x = self.add([x,inputs])\n",
    "        x = self.norm(x)\n",
    "        return x, att\n",
    "\n",
    "class PointWiseFeedForwardModule(layers.Layer):\n",
    "    def __init__(self, d_model, d_ff, do_rate):\n",
    "        super(PointWiseFeedForwardModule, self).__init__()\n",
    "        self.pwff_1 = layers.Dense(d_ff, activation='relu')\n",
    "        self.pwff_2 = layers.Dense(d_model)\n",
    "        self.dropout = layers.Dropout(do_rate)\n",
    "        self.add = layers.Add()\n",
    "        self.norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.pwff_1(inputs)\n",
    "        x = self.pwff_2(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        x = self.add([x,inputs])\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "class EncoderModule(layers.Layer):\n",
    "    def __init__(self, source_vocab_size, max_seq_len, d_model, n_heads, do_rate):\n",
    "        super(EncoderModule, self).__init__()\n",
    "        self.mhsa = MHSAModule(source_vocab_size, max_seq_len, d_model, n_heads, do_rate)\n",
    "        self.pwff = PointWiseFeedForwardModule(d_model, d_ff, do_rate)\n",
    "\n",
    "    def call(self, inputs, mask, training=False):\n",
    "        x, att = self.mhsa(inputs, mask, training)\n",
    "        x = self.pwff(x, training)\n",
    "        return x, att\n",
    "\n",
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, n_layers, source_vocab_size, max_seq_len, d_model, n_heads, do_rate):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.source_vocab_size = source_vocab_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.do_rate = do_rate\n",
    "        self.do = layers.Dropout(do_rate)\n",
    "        self.embedding = layers.Embedding(source_vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(max_seq_len, d_model)\n",
    "        self.mhsa_modules = [EncoderModule(source_vocab_size, max_seq_len, d_model, n_heads, do_rate) for _ in range(n_layers)]\n",
    "\n",
    "    def call(self, inputs, mask_enc, training=False):\n",
    "        x = self.embedding(inputs)\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.do(x, training=training)\n",
    "        attention_weights = []\n",
    "        for module in self.mhsa_modules:\n",
    "            x, att = module(x, mask_enc, training)\n",
    "            attention_weights.append(att)\n",
    "        return x, attention_weights\n",
    "\n",
    "class MHSAModuleDec(layers.Layer):\n",
    "    def __init__(self, source_vocab_size, max_seq_len, d_model, n_heads, do_rate):\n",
    "        super(MHSAModuleDec, self).__init__()\n",
    "        self.mhsa = MultiHeadSelfAttention(n_heads, d_model)\n",
    "        self.dropout = layers.Dropout(do_rate)\n",
    "        self.add = layers.Add()\n",
    "        self.norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs, context, mask, training=False):\n",
    "        x, att = self.mhsa(inputs, context, context, mask)\n",
    "        x = self.dropout(x, training=training)\n",
    "        x = self.add([x,inputs])\n",
    "        x = self.norm(x)\n",
    "        return x, att\n",
    "\n",
    "class DecoderModule(layers.Layer):\n",
    "    def __init__(self, source_vocab_size, max_seq_len, d_model, n_heads, do_rate):\n",
    "        super(DecoderModule, self).__init__()\n",
    "        self.mhsa1 = MHSAModule(source_vocab_size, max_seq_len, d_model, n_heads, do_rate)\n",
    "        self.mhsa2 = MHSAModuleDec(source_vocab_size, max_seq_len, d_model, n_heads, do_rate)\n",
    "        self.pwff = PointWiseFeedForwardModule(d_model, d_ff, do_rate)\n",
    "\n",
    "    def call(self, inputs, enc_outputs, mask, mask_look_ahead, training=False):\n",
    "        x, att1 = self.mhsa1(inputs, mask, training)\n",
    "        x, att2 = self.mhsa2(inputs, enc_outputs, mask_look_ahead, training)\n",
    "        x = self.pwff(x, training)\n",
    "        return x, att1, att2\n",
    "\n",
    "class Decoder(layers.Layer):\n",
    "    def __init__(self, n_layers, target_vocab_size, max_seq_len, d_model, n_heads, do_rate):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.target_vocab_size = target_vocab_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.do_rate = do_rate\n",
    "        self.do = layers.Dropout(do_rate)\n",
    "        self.embedding = layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(max_seq_len, d_model)\n",
    "        self.mhsa_modules = [DecoderModule(source_vocab_size, max_seq_len, d_model, n_heads, do_rate) for _ in range(n_layers)]\n",
    "\n",
    "    def call(self, inputs, enc_outputs, mask_dec, mask_look_ahead, training=False):\n",
    "        x = self.embedding(inputs)\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.do(x)\n",
    "        attention_weights = []\n",
    "        for module in self.mhsa_modules:\n",
    "            x, att1, att2 = module(x, enc_outputs, mask_dec, mask_look_ahead, training)\n",
    "            attention_weights += [[att1, att2]]\n",
    "        return x, attention_weights\n",
    "\n",
    "class Transformer(layers.Layer):\n",
    "    def __init__(self, n_layers, source_vocab_size, target_vocab_size, max_seq_len, d_model, n_heads, do_rate):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.target_vocab_size = target_vocab_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.do_rate = do_rate\n",
    "        self.encoder = Encoder(n_layers, source_vocab_size, max_seq_len, d_model, n_heads, do_rate)\n",
    "        self.decoder = Decoder(n_layers, target_vocab_size, max_seq_len, d_model, n_heads, do_rate)\n",
    "        #self.classification = layers.Dense(target_vocab_size-1, activation='softmax')\n",
    "        self.classification = layers.Dense(target_vocab_size-1)\n",
    "\n",
    "    def call(self, enc_inputs, dec_inputs, mask_enc, mask_dec, mask_look_ahead, training=False):\n",
    "\n",
    "        enc_outputs, att_enc = self.encoder(enc_inputs, mask_enc=mask_enc, training=training)\n",
    "        dec_outputs, att_dec = self.decoder(dec_inputs, enc_outputs, mask_dec=mask_dec, mask_look_ahead=mask_look_ahead, training=training)\n",
    "        y = self.classification(dec_outputs)\n",
    "        y = tf.keras.activations.softmax(y, axis=-1)\n",
    "\n",
    "        return y, att_enc + att_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3_Uh7SJF6QNG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqsW8ggu_Nhp"
   },
   "source": [
    "# 問題：\n",
    "学習済みの`transformer`層を使って、学習後に予測を行うモデルを作り翻訳させてみましょう。授業のSeq2Seqの予測の実装を理解すればほとんど同じです。ただし予測時のマスクの扱いについては注意しましょう（auto regressiveにやるので、mask_look_aheadは不要なので、ゼロテンソルを渡しましょう）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TZ7NajW1_tVe"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNqUMHiaCPjQEnefVZz4Mv7",
   "collapsed_sections": [],
   "name": "Transformer2.ipynb のコピー",
   "provenance": [
    {
     "file_id": "1kKAulaRfQDjF_OhBlECstAcy2Zt8dmAV",
     "timestamp": 1620814307021
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
