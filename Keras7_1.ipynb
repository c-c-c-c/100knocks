{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "glQNgpzUX7ac"
   },
   "source": [
    "# 7.1　Keras Functional API\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N3qn8ZHBYNMQ"
   },
   "source": [
    "## 7.1.1　Introduction\n",
    "\n",
    "Functional APIでは、tensorを直接操作し、tensorを受け取ってtensorを返す関数(function)として層を使用する。\n",
    "\n",
    "それによりFunctional APIと呼ばれる。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hN7ojnRL3xQd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import Input, layers\n",
    "\n",
    "# テンソル\n",
    "input_tensor = Input(shape=(32,))\n",
    "\n",
    "# 層は関数\n",
    "dense = layers.Dense(32, activation='relu')\n",
    "\n",
    "# テンソルで呼び出された層はテンソルを返す\n",
    "output_tensor = dense(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dMqXaLPB8Kwp"
   },
   "source": [
    "簡単な例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yZdtiZ9H4MoE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 3,466\n",
      "Trainable params: 3,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 3,466\n",
      "Trainable params: 3,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras import layers\n",
    "from keras import Input\n",
    "\n",
    "# おなじみのSequential　Model\n",
    "seq_model = Sequential()\n",
    "seq_model.add(layers.Dense(32, activation='relu', input_shape=(64,)))\n",
    "seq_model.add(layers.Dense(32, activation='relu'))\n",
    "seq_model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# Functional APIでそれに相当するもの\n",
    "input_tensor = Input(shape=(64,))\n",
    "x = layers.Dense(32, activation='relu')(input_tensor)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "output_tensor = layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "# Modelクラスは入力テンソルと出力テンソルをモデルに変換する\n",
    "model = Model(input_tensor, output_tensor)\n",
    "\n",
    "# このモデルのアーキテクチャを確認\n",
    "seq_model.summary()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8L9OrRo_AT_l"
   },
   "source": [
    "入力テンソルと出力テンソルだけを使ってModelオブジェクトをインスタンス化している。Kerasは、input_tensorからoutput_tensorまでの間にある層を全て取得し、それらをグラフ形式のデータ構造、つまりModelにまとめる。Modelのインスタンスのcompile, fit, evaluateに関してはFunctional APIはSequential modelと同じ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bn6HytCT-Zu5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 12.1669\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 13.6293\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 15.9327\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 18.9705\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 22.0116\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 25.1298\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 28.4055\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 31.7187\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 36us/step - loss: 35.4749\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 39.2936\n",
      "1000/1000 [==============================] - 0s 95us/step\n"
     ]
    }
   ],
   "source": [
    "# モデルをコンパイル\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "# 訓練に使用するダミーのNumpyデータを生成\n",
    "import numpy as np \n",
    "x_train = np.random.random((1000, 64))\n",
    "y_train = np.random.random((1000, 10))\n",
    "\n",
    "# モデルを１０エポックで訓練\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=128)\n",
    "\n",
    "# モデルを評価\n",
    "score = model.evaluate(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HgEe5HDhYiqb"
   },
   "source": [
    "## 7.1.2　多入力モデル\n",
    "\n",
    "・複数の入力を持つモデルの構築が可能\n",
    "\n",
    "\n",
    "・一般にはkeras.layers.addやkeras.layers.concatenateで入力分岐をマージする"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iRavj5yVZoLr"
   },
   "source": [
    "### list 7-1: 2 input\n",
    "\n",
    "・2つの入力を持つ質問応答モデルの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MGaqD98EZ0t4"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'output_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-0ef0fdf2d11d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# 入力をサイズが６４のベクトルシーケンスに埋め込む\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0membedded_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_vocabulary_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# LSTMを通じてこれらのベクトルを単一のベクトルにエンコード\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'output_dim'"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras import layers\n",
    "from keras import Input\n",
    "\n",
    "text_vocabulary_size = 10000\n",
    "question_vocabulary_size = 10000\n",
    "answer_vocabulary_size = 500\n",
    "\n",
    "# テキスト入力は整数の可変長のシーケンス\n",
    "# なお、必要であれば、入力に名前をつけることもできる\n",
    "text_input = Input(shape=(None,), dtype='int32', name='text')\n",
    "\n",
    "# 入力をサイズが６４のベクトルシーケンスに埋め込む\n",
    "embedded_text = layers.Embedding(text_vocabulary_size)(text_input, 64)\n",
    "\n",
    "# LSTMを通じてこれらのベクトルを単一のベクトルにエンコード\n",
    "encorded_text = layers.LSTM(32)(embedded_text)\n",
    "\n",
    "# 質問入力でも（異なる層のインスタンスを使って）同じプロセスを繰り返す\n",
    "question_input = Input(shape=(None,), dtype='int32', name='question')\n",
    "embedded_question = layers.Embedding(question_vocabulary_size, 32)(question_input)\n",
    "encoded_question = layers.LSTM(16)(embedded_question)\n",
    "\n",
    "# エンコードされたテキストと質問を連結\n",
    "concatenated = layers.concatenate([encoded_text, encoded_question], axis=-1)\n",
    "\n",
    "#　ソフトマックス分類器を追加\n",
    "answer = layers.Dense(answer_vocabulary_size, activation='softmax')(concatenate)\n",
    "\n",
    "# モデルをインスタンス化するときには、2つの入力と1つの出力を指定\n",
    "model = Model([text_input, question_input], answer)\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PEa3GM3ZpGDJ"
   },
   "source": [
    "### list 7-2: data for multi-input model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R_buiq_8pGfY"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "num_samples = 1000\n",
    "max_length = 100\n",
    "\n",
    "text_vocabulary_size = 10000\n",
    "question_vocabulary_size = 10000\n",
    "answer_vocabulary_size = 500\n",
    "\n",
    "# ダミーのNumpyデータを生成\n",
    "text = np.random.randint(1, text_vocabulary_size,\n",
    "                         size=(num_samples, max_length))\n",
    "question = np.random.randint(1, question_vocabulary_size, \n",
    "                             size=(num_samples, max_length))\n",
    "\n",
    "#　答えに（整数ではなく）one-hotエンコーディングを適用\n",
    "answers = np.zeros(shape=(num_samples, answer_vocabulary_size))\n",
    "indices = np.random.randint(0, answer_vocabulary_size, size = num_samples)\n",
    "for i, x in enumerate(answers):\n",
    "  x[indices[i]] = 1\n",
    "\n",
    "#　入力リストを使った適合\n",
    "model.fit([text, question], answers, epochs=10, batch_size=128)\n",
    "\n",
    "#　入力ディクショナリを使った適合（入力に名前を付ける場合のみ）\n",
    "model.fit({'text': text, 'question': question}, answers, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KDALT0weYsXf"
   },
   "source": [
    "## 7.1.3　多出力モデル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wp2cPNuMuOgl"
   },
   "source": [
    "### list 7-3：3 output\n",
    "\n",
    "Functional APIを使って複数の出力を持つモデルを構築できる。例えば、同一の匿名ユーザによるSNSの一連の投稿を入力として、そのユーザーの年齢、性別、所得水準といった複数の属性を予測するネットワークが考えられる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k6p6TL5AuNIB"
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import Input\n",
    "from keras.models import Model\n",
    "\n",
    "vocabulary_size = 50000\n",
    "num_income_groups = 10\n",
    "\n",
    "posts_input = Input(shape=(None,), dtype='int32', name='posts')\n",
    "embedded_posts = layers.Embedding(256, vocabulary_size)(posts_input)\n",
    "x = layers.Conv1D(128, 5, activation='relu')(embedded_posts)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(256, 5, activation='relu')(x)\n",
    "x = layers.Conv1D(256, 5, activation='relu')(x)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(256, 5, activation='relu')(x)\n",
    "x = layers.Conv1D(256, 5, activation='relu')(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "\n",
    "#　出力層に名前が付いていることに注意\n",
    "age_prediction = layers.Dense(1, name='age')(x)\n",
    "income_prediction = layers.Dense(num_income_groups,\n",
    "                                 activation='softmax',\n",
    "                                 name='income')(x)\n",
    "gender_prediction = layers.Dense(1, activation='sigmoid', name='gender')(x)\n",
    "model = Model(posts_input,\n",
    "              [age_prediction, income_prediction, gender_prediction])                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xpRjReE9RiiW"
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hzgLUHyUwuYm"
   },
   "source": [
    "### list 7-4： compile option (multi-loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ib-ZuSKDw1mp"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss=['mse',\n",
    "                    'categorical_crossentropy',\n",
    "                    'binary_crossentropy'])\n",
    "\n",
    "#　上記と同じ（出力層に名前をつけている場合にのみ可能）\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss={'age':'mse',\n",
    "                    'income': 'categorical_crossentropy',\n",
    "                    'gender': 'binary_crossentropy'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BZxV0uoKO9Tg"
   },
   "source": [
    "### list 7-5： compile option (loss_weights)\n",
    "\n",
    "損失値の貢献度がかなり不均衡である場合、損失値に対して重要度を割り当てる。特に損失値の尺度が異なる場合、例えば年齢回帰タスクに使用されるMSEは一般に3〜5の値をとるが、性別分類タスクに使用される交差エントロピーの値は最低でも0.1になることがある。よって重要度のバランスをとるため、MSEに0.25の重みを、交差エントロピーに10の重みを割り当てることができる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TqVs8JFpzWTo"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "When passing a list as loss, it should have one entry per model outputs. The model has 1 outputs, but you passed loss=['mse', 'categorical_crossentropy', 'binary_crossentropy']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-bff1abdb92e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m               loss_weights=[0.25, \n\u001b[1;32m      6\u001b[0m                             \u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                             10.])\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#　上記と同じ（出力層に名前をつけている場合にのみ可能）\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# Prepare list of loss functions, same size as model outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         self.loss_functions = training_utils.prepare_loss_functions(\n\u001b[0;32m--> 119\u001b[0;31m             self.loss, self.output_names)\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mprepare_loss_functions\u001b[0;34m(loss, output_names)\u001b[0m\n\u001b[1;32m    825\u001b[0m             raise ValueError('When passing a list as loss, it should have one entry '\n\u001b[1;32m    826\u001b[0m                              \u001b[0;34m'per model outputs. The model has {} outputs, but you '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 827\u001b[0;31m                              'passed loss={}'.format(len(output_names), loss))\n\u001b[0m\u001b[1;32m    828\u001b[0m         \u001b[0mloss_functions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget_loss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: When passing a list as loss, it should have one entry per model outputs. The model has 1 outputs, but you passed loss=['mse', 'categorical_crossentropy', 'binary_crossentropy']"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss=['mse', \n",
    "                    'categorical_crossentropy',\n",
    "                    'binary_crossentropy'],\n",
    "              loss_weights=[0.25, \n",
    "                            1., \n",
    "                            10.])\n",
    "\n",
    "#　上記と同じ（出力層に名前をつけている場合にのみ可能）\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss={'age':'mse',\n",
    "                    'income': 'categorical_crossentropy',\n",
    "                    'gender': 'binary_crossentropy'},\n",
    "              loss_weights={'age':0.25, \n",
    "                            'income':1., \n",
    "                            'gender':10.})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VRWNUJCzPdyU"
   },
   "source": [
    "### list 7-6： data for multi-output model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mtdie3340YEc"
   },
   "outputs": [],
   "source": [
    "#　age_targets,　income_targets, gender_targetsはNumPy配列と仮定\n",
    "model.fit(posts, [age_targets,　income_targets, gender_targets],\n",
    "           epochs=10, batch_size=64)\n",
    "\n",
    "#　上記と同じ（出力層に名前をつけている場合にのみ可能）\n",
    "model.fit(posts, {'age': age_targets,\n",
    "                  'income': income_targets,\n",
    "                  'gender': gender_targets},\n",
    "          epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L1tFEHpNYxwB"
   },
   "source": [
    "## 7.1.4　層の有向非巡回グラフ\n",
    "\n",
    "・Directed Acyclic Graph\n",
    "\n",
    "・Inception Module\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rQGSPwwt-zGr"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "### **WHY 1x1 convolution?**\n",
    "\n",
    "\n",
    "畳み込みでは、入力テンソルの全てのタイルのまわりで空間パッチが抽出され、全てのパッチに同じ変換が適用されます。エッジケースは、抽出されたパッチがたった1つのタイルで構成されている場合です。その場合、畳み込み演算はDense層を通じて各タイルベクトルを実行することに等しくなります。つまり、入力テンソルの各チャネルの情報を混ぜ合わせた特徴量を計算することになりますが、（一度に調べるタイルは2つだけなので）空間にまたがって情報を混ぜ合わせることはありません。Inceptionモジュールでは、そうした１x１の畳み込みが挿入され、**チャネルごとの表現学習と空間ごとの表現学習の因数分解に貢献します**。１x１の畳み込みは、pw畳み込み（pointwise convolution）とも呼ばれます。このオプションは、各チャネルが空間を跨いで強い自己相関関係にある場合には合理的なのですが、チャネルによっては相互の関連性が低いことも考えられます。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jIQAWNY-B0J0"
   },
   "source": [
    "**Inceptionモジュール**　（４次元の入力テンソルxが存在する前提）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "kQZFXovEIlts"
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import Input\n",
    "import numpy as np\n",
    "\n",
    "branch_a = layers.Conv2D(128, 1, activation='relu', strides=2)(x)\n",
    "\n",
    "branch_b = layers.Conv2D(128, 1, activation='relu')(x)\n",
    "branch_b = layers.Conv2D(128, 3, activation='relu', strides=2)(branch_b)\n",
    "\n",
    "branch_c = layers.AveragePooling2D(3, strides=2)(x)\n",
    "branch_c = layers.Conv2D(128, 3, activation='relu')(branch_c)\n",
    "\n",
    "branch_d = layers.Conv2D(128, 1, activation='relu')(x)\n",
    "branch_d = layers.Conv2D(128, 3, activation='relu')(branch_d)\n",
    "branch_d = layers.Conv2D(128, 3, activation='relu', strides=2)(branch_d)\n",
    "\n",
    "output = layers.concatenate([branch_a, branch_b, branch_c, branch_d], axis=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oSY-CNs_iEK2"
   },
   "source": [
    "**残差接続**(４次元の入力テンソルxが存在する前提)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7eTl5AFbjif5"
   },
   "outputs": [],
   "source": [
    "# 特徴マップのサイズが同じ場合\n",
    "\n",
    "x = .....\n",
    "\n",
    "# xに変換を適用\n",
    "y = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "y = layers.Conv2D(128, 3, activation='relu', padding='same')(y)\n",
    "y = layers.Conv2D(128, 3, activation='relu', padding='same')(y)\n",
    "\n",
    "# 元のxを出力特徴量に追加\n",
    "y = layers.add([y, x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yFisQncukQRw"
   },
   "outputs": [],
   "source": [
    "# 特徴マップのサイズが異なっている場合に、線形残差接続を使用する実装\n",
    "\n",
    "x = .....\n",
    "\n",
    "y = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "y = layers.Conv2D(128, 3, activation='relu', padding='same')(y)\n",
    "y = layers.MaxPooling2D(2, strides=2)(y)\n",
    "\n",
    "# 元のテンソルxをyと同じ形状にするための１x１畳み込みを使った線形ダウンサンプリング\n",
    "residual = layers.Conv2D(128, 1, strides=2, padding='same')(x)\n",
    "\n",
    "# 残差テンソルを出力特徴量に追加\n",
    "y = layers.add([y, residual])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BGYG-PtiYA1z"
   },
   "source": [
    "### **DLの表現のBottleneck**\n",
    "\n",
    "Sequentialモデルでは、連続する表現層はそれぞれ前の層の上に構築されます。つまり、その層からアクセスできる情報は、前の層の活性化に含まれているものだけです。1つの層が小さすぎる場合（特徴量の次元の数が少なすぎるなど）、モデルはその層の活性化に詰め込むことができる情報の量によって制限されることになります。\n",
    "\n",
    "この概念を理解するために、信号処理についての考えてみましょう。一連の演算で構成された音声処理のパイプラインがあり、それぞれの演算の入力は1つ前の演算の出力です。ある演算が信号を低周波域（0〜15kHzなど）にクロッピングした場合、下流の演算は削除された周波数を復元できなくなります。情報の損失は全て恒久的です。残差接続は、上流の情報を下流に再注入することで、ディープラーニングモデルのこの問題を部分的に解決します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4-kkOn4JZbTq"
   },
   "source": [
    "### **DLでの勾配消失問題**\n",
    "\n",
    "バックプロパゲーション（誤差逆伝播法）は、ディープニューラルネットワークの訓練に使用されるマスターアルゴリズムです。このアルゴリズムは、出力側の損失関数からのフィードバック信号を入力側の層へ伝播するという仕組みになっています。このフィードバック信号を伝播させる層が深いスタックになっている場合、フィードバック信号が弱められたり完全に失われたりして、ネットワークを訓練できなくなる可能性があります。この問題は**勾配消失**（vanishing gradient）と呼ばれます。\n",
    "\n",
    "この問題が発生するのは、ディープニューラルネットワーク（DNN）と、非常に長いシーケンスを処理するリカレントニューラルネットワーク（RNN）です。どちらの場合もフィードバック信号は演算の長い連鎖に渡って伝播されなければなりません。既に説明したように、RNNではLSTMを使ってこの問題に対処します。それにより、メインの処理トラックに情報を同時に運び込むキャリートラックが追加されます。フィードフォワードDNNでは、残差接続が同じような役割を果たしますが、この場合はさらに単純です。残差接続により、スタックの深さに関係なく勾配を伝播させるのに役立つ、完全に線形のキャリートラックが追加されるからです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ygureio0ZAXr"
   },
   "source": [
    "## 7.1.5　層の重みの共有\n",
    "\n",
    "Siamese Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LlfYt7BlyByi"
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import Input\n",
    "from keras.models import Model\n",
    "\n",
    "# 単一のLSTM層を一度だけインスタンス化\n",
    "lstm = layers.LSTM(32)\n",
    "\n",
    "# モデルの左側の分岐を構築\n",
    "left_input = Input(shape=(None, 128))\n",
    "left_output = lstm(left_input)\n",
    "\n",
    "# モデルの右側の分岐を構築\n",
    "right_input = Input(shape=(None, 128))\n",
    "right_output = lstm(right_input)\n",
    "\n",
    "# 最後に分類器を構築\n",
    "merged = layers.concatenate([left_output, right_output], axis=-1)\n",
    "predictions = layers.Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "# モデルのインスタンス化と訓練\n",
    "# このようなモデルを訓練する時には、LSTM層の重みが両方の入力に基づいて更新される\n",
    "model = Model([left_input, right_input], predictions)\n",
    "model.fit([left_input, right_input], targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8qrL0lmYZU21"
   },
   "source": [
    "## 7.1.6　層としてのモデル\n",
    "\n",
    "・層を使用するときのようにモデルを使用できる。\n",
    "\n",
    "・モデルをより大きな層として考えることができる。\n",
    "\n",
    "・入力テンソルを使ってモデルを呼び出し、出力テンソルを取得することが可能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xM0BLFeDmN3N"
   },
   "source": [
    "\n",
    "入力テンソルを使ってモデルを呼び出し、出力テンソルを取得することが可能\n",
    "```\n",
    "y = model(x)\n",
    "```\n",
    "\n",
    "入力テンソルと出力テンソルが複数の場合\n",
    "\n",
    "```\n",
    "y1, y2 = model([x1, x2])\n",
    "  ```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_1KSxOoNr5of"
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import applications\n",
    "from keras import Input\n",
    "\n",
    "# ベースとなる画像処理モデルはXceptionネットワーク（畳み込みベースのみ）\n",
    "xception_base = applications.Xception(weights=None, include_top=False)\n",
    "\n",
    "# 入力は250x250のRGB画像\n",
    "left_input = Input(shape=(250, 250, 3))\n",
    "right_input = Input(shape=(250, 250, 3))\n",
    "\n",
    "# 同じビジョンモデルを２回呼び出す\n",
    "left_features = xception_base(left_input)\n",
    "right_input = xception_base(right_input)\n",
    "\n",
    "# マージ後の特徴量には、右の視覚フィードと左の視覚フィードの情報が含まれている\n",
    "merged_features = layers.concatenate([left_features, right_input], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I41duQ9oZfQA"
   },
   "source": [
    "## 7.1.7　まとめ\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "・層の線形スタック以外のものが必要な場合に使用する方法。\n",
    "\n",
    "・複数の入出力、複雑な内部ネットワークトポロジを持つモデルを構築する方法。\n",
    "\n",
    "・同じ層またはモデルのインスタンスを複数回呼び出すことで、様々な処理分岐にまたがって層やモデルの重みを再利用する方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xQSMcKTAX4hW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Keras7-1.ipynb のコピー",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
